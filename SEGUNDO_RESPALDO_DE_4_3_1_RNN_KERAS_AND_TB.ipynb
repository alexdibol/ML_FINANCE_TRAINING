{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEGUNDO RESPALDO DE  4_3_1 RNN KERAS AND TB",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexdibol/ML_FINANCE_TRAINING/blob/master/SEGUNDO_RESPALDO_DE_4_3_1_RNN_KERAS_AND_TB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRDTCjAMIJLx",
        "colab_type": "text"
      },
      "source": [
        "# RECURRENT NEURAL NETWORKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jfOdaQLhXLDR"
      },
      "source": [
        "## 1. MOTIVATION\n",
        "\n",
        "Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\n",
        "\n",
        "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n",
        "\n",
        "The Keras RNN API is designed with a focus on:\n",
        "\n",
        "- **Ease of use**: the built-in `tf.keras.layers.RNN`, `tf.keras.layers.LSTM`, `tf.keras.layers.GRU` layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n",
        "  \n",
        "- **Ease of customization**: You can also define your own RNN cell layer (the inner part of the `for` loop) with custom behavior, and use it with the generic `tf.keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDiecnGr7RTq",
        "colab_type": "text"
      },
      "source": [
        "## 2. A SIMPLE MODEL WITH AN EMBEDDING LAYER\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb#scrollTo=SIXEk5ON5P7h"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SIXEk5ON5P7h",
        "colab": {}
      },
      "source": [
        "#!pip install tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RutaI-Tpev3T",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTucKgHe9F__",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. GENERAL CONCEPT: INCLUDING THE EMBEDDING LAYER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2dKKV1L2Rk7e"
      },
      "source": [
        "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n",
        "\n",
        "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O4PC4QzsxTGx"
      },
      "source": [
        "For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15).\n",
        "\n",
        "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6g505Fcgs",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. LOADING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yg6tyxPtp1TE",
        "colab": {}
      },
      "source": [
        "(train_data, test_data), info = tfds.load(\n",
        "    'imdb_reviews/subwords8k', \n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
        "    with_info=True, as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17sN1m7PFwQG",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. PREPARING THE DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jjnBsFXaLVPL"
      },
      "source": [
        "Get the encoder (`tfds.features.text.SubwordTextEncoder`), and have a quick look at the vocabulary. \n",
        "\n",
        "The \"\\_\" in the vocabulary represent spaces. Note how the vocabulary includes whole words (ending with \"\\_\") and partial words which it can use to build larger words:\n",
        "\n",
        "Movie reviews can be different lengths. We will use the padded_batch method to standardize the lengths of the reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi2zw-ZK7JwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = info.features['text'].encoder\n",
        "#encoder.subwords[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LwSCxER_2Lef",
        "colab": {}
      },
      "source": [
        "train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))\n",
        "test_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8qCspBoRZKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(train_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Viwdu98XF3UW",
        "colab_type": "text"
      },
      "source": [
        "## 3. SETTING UP THE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwQCU5pINxJX",
        "colab_type": "text"
      },
      "source": [
        "We will use the Keras Sequential API to define our model. In this case it is a \"Continuous bag of words\" style model.\n",
        "\n",
        "Next the Embedding layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n",
        "\n",
        "Next, a GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "\n",
        "This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
        "\n",
        "The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability (or confidence level) that the review is positive.\n",
        "\n",
        "Caution: This model doesn't use masking, so the zero-padding is used as part of the input, so the padding length may affect the output. To fix this, see the masking and padding guide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO73ytA47JoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim=16\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(16, activation='relu'),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lCUgdP69Wzix",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=10,\n",
        "    verbose=0, # Suppress chatty output\n",
        "    validation_data=test_batches, validation_steps=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMAnayGpSAjI",
        "colab_type": "text"
      },
      "source": [
        "## 4. PLOTTING THE RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LQjpKVYTXU-1"
      },
      "source": [
        "With this approach our model reaches a validation accuracy of around 88% (note the model is overfitting, training accuracy is significantly higher)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0D3OTmOT1z1O",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss=history_dict['loss']\n",
        "val_loss=history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs, loss, 'b', label='Training loss', color='magenta')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs, acc, 'b', label='Training acc', color='magenta')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim((0.5,1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEAaCg4YZZwg",
        "colab_type": "text"
      },
      "source": [
        "## 5. PREPARE FOR EXTERNAL TENSORBOARD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UxS6mj5qZW0h",
        "colab": {}
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrBrwwMjYp8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "encoder = info.features['text'].encoder\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate(encoder.subwords):\n",
        "  vec = weights[num+1] # skip 0, it's padding.\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqmqvDTJfHBU",
        "colab_type": "text"
      },
      "source": [
        "To visualize our embeddings we will upload them to the embedding projector.\n",
        "\n",
        "Open the Embedding Projector (this can also run in a local TensorBoard instance).\n",
        "\n",
        "- Click on \"Load data\".\n",
        "\n",
        "- Upload the two files we created above: vecs.tsv and meta.tsv.\n",
        "\n",
        "- The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\".\n",
        "\n",
        "**Note: your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer.**\n",
        "\n",
        "`Note: experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the Dense(16) layer, retraining the model, and visualizing the embeddings again.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkxQAJ5bZLLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E8bVaO9aUQz",
        "colab_type": "text"
      },
      "source": [
        "USE THE EXTERNAL PROJECTOR\n",
        "\n",
        "http://projector.tensorflow.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqfIejCvV6Jn",
        "colab_type": "text"
      },
      "source": [
        "## 6.  THE 1-2-3 FOR USING THE BUILT IN TENSORBOARD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV3Ww00J2vjg",
        "colab_type": "text"
      },
      "source": [
        "### STEP 1. LOADING THE PLUG-IN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjpIGowP5-lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from tensorboard.plugins import projector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqEHjAM33BZB",
        "colab_type": "text"
      },
      "source": [
        "### STEP 2. CREATION OF DIRECTORY, WRITER AND CALLBACK FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI204-RoCofs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOGDIR='content/logs/imdb-example/'\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=LOGDIR)\n",
        "file_writer = tf.summary.create_file_writer(LOGDIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOO0b6xZ3TvH",
        "colab_type": "text"
      },
      "source": [
        "### STEP 3. COMPILATION AND EXECUTION OF THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8klBB7pVCpwi",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=10,\n",
        "    verbose=0, # Suppress chatty output\n",
        "    callbacks=[tensorboard_callback],\n",
        "    validation_data=test_batches, validation_steps=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRAbw4wOEPPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir 'content/logs/imdb-example/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tGRz2_7C6AO",
        "colab_type": "text"
      },
      "source": [
        "## 7. THE 1-2-3 OF INCLUDING EMBEDDINGS WITH TENSORBOARD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiLrfO29YBux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import os\n",
        "#from tensorboard.plugins import projector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thR4Gi78C55c",
        "colab_type": "text"
      },
      "source": [
        "### STEP 1. SAVING THE LABELS: METDATA.TSV FILE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHWHqRzO-A8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Labels separately on a line-by-line manner.\n",
        "with open(os.path.join(LOGDIR, 'metadata.tsv'), \"w\") as f:\n",
        "  for subwords in encoder.subwords:\n",
        "    f.write(\"{}\\n\".format(subwords))\n",
        "  # Fill in the rest of the labels with \"unknown\"\n",
        "  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
        "    f.write(\"unknown #{}\\n\".format(unknown))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLpoezNWmlB",
        "colab_type": "text"
      },
      "source": [
        "### STEP 2. RETRIEVE AND SAVE WEIGHTS OF THE FIRST (EMBEDDING) LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s7Lomyc-KOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the weights we want to analyse as a variable. Note that the first\n",
        "weights = tf.Variable(model.layers[0].get_weights()[0][1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnEBHxWcYL0Z",
        "colab_type": "text"
      },
      "source": [
        "### STEP 3. CREATE A CHECKPOINT AND CONFIGURE THE PROJECTOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwnXuVae-TAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a checkpoint from embedding, the filename and key are\n",
        "# name of the tensor.\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(LOGDIR, \"embedding.ckpt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDgLBeWp2HDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up config\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(LOGDIR, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upAVoDae5HB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir 'content/logs/imdb-example/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc_XngUDa5Z5",
        "colab_type": "text"
      },
      "source": [
        "## 8. INCLUDING A RECURRENT LAYER IN THE NEURAL NETWORK\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/rnn.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "he462AuJawEi",
        "colab": {}
      },
      "source": [
        "embedding_dim=16\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
        "  #layers.SimpleRNN(1),\n",
        "  #layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(16, activation='relu'),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFst5JVNdpnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=3,\n",
        "    verbose=0, # Suppress chatty output\n",
        "    callbacks=[tensorboard_callback],\n",
        "    validation_data=test_batches, validation_steps=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltWQ4Z_qdqxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNZf6AfA2FUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}