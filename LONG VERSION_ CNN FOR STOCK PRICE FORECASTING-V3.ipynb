{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  BITCOIN PRICE FORECASTING </center></h1>\n",
    "\n",
    "<h2><center>LONG VERSION</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ONE-STEP CODE FOR A 15-EPOCH RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "961/961 [==============================] - 715s 744ms/step - loss: 0.7154 - accuracy: 0.5218 - val_loss: 0.6980 - val_accuracy: 0.5420\n",
      "Epoch 2/15\n",
      "961/961 [==============================] - 708s 736ms/step - loss: 0.6900 - accuracy: 0.5448 - val_loss: 0.7052 - val_accuracy: 0.5270\n",
      "Epoch 3/15\n",
      "961/961 [==============================] - 707s 736ms/step - loss: 0.6844 - accuracy: 0.5588 - val_loss: 0.7104 - val_accuracy: 0.5331\n",
      "Epoch 4/15\n",
      "961/961 [==============================] - 709s 738ms/step - loss: 0.6743 - accuracy: 0.5821 - val_loss: 0.7131 - val_accuracy: 0.5612\n",
      "Epoch 5/15\n",
      "961/961 [==============================] - 708s 737ms/step - loss: 0.6468 - accuracy: 0.6224 - val_loss: 0.6909 - val_accuracy: 0.5309\n",
      "Epoch 6/15\n",
      "961/961 [==============================] - 708s 737ms/step - loss: 0.5910 - accuracy: 0.6786 - val_loss: 0.7778 - val_accuracy: 0.5347\n",
      "Epoch 7/15\n",
      "961/961 [==============================] - 709s 738ms/step - loss: 0.5149 - accuracy: 0.7374 - val_loss: 0.7660 - val_accuracy: 0.5417\n",
      "Epoch 8/15\n",
      "961/961 [==============================] - 708s 737ms/step - loss: 0.4353 - accuracy: 0.7893 - val_loss: 0.8848 - val_accuracy: 0.5259\n",
      "Epoch 9/15\n",
      "961/961 [==============================] - 708s 737ms/step - loss: 0.3616 - accuracy: 0.8325 - val_loss: 0.9518 - val_accuracy: 0.5298\n",
      "Epoch 10/15\n",
      "961/961 [==============================] - 708s 737ms/step - loss: 0.3011 - accuracy: 0.8659 - val_loss: 0.8680 - val_accuracy: 0.5249\n",
      "Epoch 11/15\n",
      "961/961 [==============================] - 644s 670ms/step - loss: 0.2521 - accuracy: 0.8907 - val_loss: 1.1526 - val_accuracy: 0.5214\n",
      "Epoch 12/15\n",
      "961/961 [==============================] - 579s 602ms/step - loss: 0.2137 - accuracy: 0.9096 - val_loss: 1.2675 - val_accuracy: 0.5327\n",
      "Epoch 13/15\n",
      "961/961 [==============================] - 592s 616ms/step - loss: 0.1835 - accuracy: 0.9243 - val_loss: 1.1596 - val_accuracy: 0.5190\n",
      "Epoch 14/15\n",
      "961/961 [==============================] - 585s 609ms/step - loss: 0.1599 - accuracy: 0.9361 - val_loss: 1.1452 - val_accuracy: 0.5249\n",
      "Epoch 15/15\n",
      "961/961 [==============================] - 562s 585ms/step - loss: 0.1425 - accuracy: 0.9434 - val_loss: 1.5521 - val_accuracy: 0.5172\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def exponential_moving_average(ps, look_back):\n",
    "    return ps.ewm(span=look_back, min_periods=0, adjust=False, ignore_na=True).mean()\n",
    "\n",
    "\n",
    "def macd(ps, short, long):\n",
    "    short_ema = exponential_moving_average(ps, short)\n",
    "    long_ema = exponential_moving_average(ps, long)\n",
    "    return short_ema - long_ema\n",
    "\n",
    "\n",
    "def log_difference(ps):\n",
    "    return log_momentum(ps, 1)\n",
    "\n",
    "\n",
    "def log_momentum(ps, m):\n",
    "    log_price = np.log(ps)\n",
    "    shifted_log_price = log_price.shift(m)\n",
    "    return log_price - shifted_log_price\n",
    "\n",
    "\n",
    "class MovingWindowGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, window_size, matrix, target, window_index_list, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.window_index_list = window_index_list\n",
    "        self.example_index_list = np.arange(len(self.window_index_list))\n",
    "        self.matrix = matrix\n",
    "        self.target = target\n",
    "        self.window_size = window_size  # The size of the moving window\n",
    "        self.on_epoch_end()  # Randomize the sample order\n",
    "        self.classes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.window_index_list) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example_indices_for_current_batch = self.example_index_list[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self.__data_generation(example_indices_for_current_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.example_index_list)\n",
    "\n",
    "    def __data_generation(self, example_indexs_for_current_batch):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        n = self.window_size\n",
    "\n",
    "        for sample_index in example_indexs_for_current_batch:\n",
    "            window_index = self.window_index_list[sample_index]\n",
    "            window = self.matrix[window_index-n+1:window_index+1]  # Extract the window\n",
    "            target = self.target[window_index]  # Extract the target(y)\n",
    "            X.append(window)\n",
    "            y.append(target)\n",
    "\n",
    "        self.classes += y\n",
    "\n",
    "        # Convert python lists to numpy arrays\n",
    "        X, y = np.stack(X), np.stack(y)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def add_fully_connected_layers(model, hyperparameter_dict):\n",
    "    fc_layers = hyperparameter_dict['fc_layers']\n",
    "    activation = hyperparameter_dict['activation']\n",
    "    n_classes = hyperparameter_dict['n_classes']\n",
    "\n",
    "    if fc_layers is not None:\n",
    "        for layer in fc_layers:\n",
    "            model.add(Dense(layer, activation=activation))\n",
    "            if 'dropout' in hyperparameter_dict:\n",
    "                model.add(Dropout(hyperparameter_dict['dropout']))\n",
    "\n",
    "            if 'batch_normalization' in hyperparameter_dict:\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "    if n_classes > 2:\n",
    "        model.add(Dense(n_classes, activation='softmax'))\n",
    "    elif n_classes == 2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    else:  # Regression\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_simple_CNN(hyperparameter_dict):\n",
    "    model = Sequential()\n",
    "    kernel_size = hyperparameter_dict['kernel_size']\n",
    "    n_filters = hyperparameter_dict['n_filters']\n",
    "    n_layers = hyperparameter_dict['n_cnn_layers']\n",
    "    n_blocks = hyperparameter_dict['n_cnn_blocks']\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "        for j in range(n_layers):\n",
    "            params = {\n",
    "                \"filters\": n_filters,\n",
    "                \"kernel_size\": kernel_size,\n",
    "                \"activation\": hyperparameter_dict['activation'],\n",
    "                \"padding\": 'same'\n",
    "            }\n",
    "\n",
    "            if i == 0 and j == 0:  # Handle the first layer specially\n",
    "                params['input_shape'] = hyperparameter_dict['input_dim']\n",
    "\n",
    "            model.add(Conv1D(**params))\n",
    "\n",
    "            if 'cnn_dropout' in hyperparameter_dict:\n",
    "                model.add(Dropout(hyperparameter_dict['cnn_dropout']))\n",
    "\n",
    "            if 'cnn_bn' in hyperparameter_dict:\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "        if 'pool_size' in hyperparameter_dict:\n",
    "            model.add(MaxPooling1D(pool_size=hyperparameter_dict['pool_size']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    add_fully_connected_layers(model, hyperparameter_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def balance_and_generate_index_lists(sequence, target_series, window_size):\n",
    "    window_index_dictionary = defaultdict(list)\n",
    "    unbalanced_window_index_list = []\n",
    "    downsampled_window_index_list = []\n",
    "\n",
    "    # Populate the dictionary with window indices\n",
    "    for i in range(window_size - 1, len(sequence) - window_size + 1):\n",
    "        window_index_dictionary[target_series[i]].append(i)\n",
    "\n",
    "    # Create an unbalanced version of the index list\n",
    "    for cls in window_index_dictionary:\n",
    "        unbalanced_window_index_list += window_index_dictionary[cls]\n",
    "\n",
    "    # Figure out the least frequent class\n",
    "    lengths = [len(index_list_for_class) for index_list_for_class in window_index_dictionary.values()]\n",
    "    min_length = int(np.min(lengths))\n",
    "\n",
    "    # Undersampling happens here\n",
    "    for cls in window_index_dictionary:\n",
    "        random.shuffle(window_index_dictionary[cls])\n",
    "        window_index_dictionary[cls] = window_index_dictionary[cls][:min_length]\n",
    "\n",
    "    # Concatenate the list\n",
    "    for cls in window_index_dictionary:\n",
    "        downsampled_window_index_list += window_index_dictionary[cls]\n",
    "\n",
    "    return downsampled_window_index_list, unbalanced_window_index_list\n",
    "\n",
    "\n",
    "hyperparameter_dict = {\n",
    "    'kernel_size': 5,\n",
    "    'dropout': 0.2,\n",
    "    'n_classes': 2,\n",
    "    'window_size': 100,\n",
    "    'validation_ratio': 0.25,\n",
    "    'batch_size': 512,\n",
    "    'n_epochs': 15,\n",
    "    'monitor': 'val_loss',\n",
    "    'patience': 5,\n",
    "\n",
    "    'forecast_horizon': 10,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    # model-specific hyper parameters\n",
    "    'model': 'stacked_CNN',\n",
    "    'n_cnn_layers': 2,\n",
    "    'n_cnn_blocks': 1,\n",
    "    'cnn_bn': True,\n",
    "    'batch_normalization': True,\n",
    "    'n_filters': 64,\n",
    "    'fc_layers': [1024, 1024],\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "# Extracting values from the hyperparameter dictionary\n",
    "window_size = hyperparameter_dict['window_size']\n",
    "forecast_horizon = hyperparameter_dict['forecast_horizon']\n",
    "n_epochs = hyperparameter_dict['n_epochs']\n",
    "n_classes = hyperparameter_dict['n_classes']\n",
    "validation_ratio = hyperparameter_dict['validation_ratio']\n",
    "\n",
    "df = pd.read_csv('NEOBTC.csv', index_col='timestamp', parse_dates=True)\n",
    "\n",
    "# Optional Indicator Calculation\n",
    "price = df['close']\n",
    "df['MACD'] = macd(price, 10, 20)\n",
    "df['log_difference'] = log_difference(price)\n",
    "df['log_momentum'] = log_momentum(price, 20)\n",
    "\n",
    "# Target Generation\n",
    "df['shifted_close'] = df['close'].shift(-forecast_horizon)  # The shifted version will have a couple of NaNs\n",
    "df.dropna(inplace=True)  # Drop the NaNs\n",
    "df['target'] = (df['shifted_close'] - df['close'] > 0).astype(int)\n",
    "df.drop(['shifted_close'], axis=1, inplace=True)  # We don't need this column any more\n",
    "\n",
    "# Isolate the target series\n",
    "target_series = df['target'].values  # Extract the numpy array inside\n",
    "df.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "# Split\n",
    "n_train = len(df) - int(validation_ratio * len(df))\n",
    "train_df, validation_df = df[:n_train], df[n_train:]\n",
    "train_target, validation_target = target_series[:n_train], target_series[n_train:]\n",
    "\n",
    "n_features = train_df.shape[1]\n",
    "\n",
    "# Standardize the matrices\n",
    "scaler = StandardScaler()\n",
    "train_sequence = scaler.fit_transform(train_df)\n",
    "validation_sequence = scaler.transform(validation_df)\n",
    "\n",
    "# Downsample and generate moving window indices\n",
    "train_indices_downsampled, train_indices_unbalanced = balance_and_generate_index_lists(train_sequence,\n",
    "                                                                                       train_target,\n",
    "                                                                                       window_size)\n",
    "validation_indices_downsampled, validation_indices_unbalanced = balance_and_generate_index_lists(validation_sequence,\n",
    "                                                                                                 train_target,\n",
    "                                                                                                 window_size)\n",
    "\n",
    "# Instantiate the moving window generators for each partition\n",
    "train_gen = MovingWindowGenerator(batch_size=hyperparameter_dict['batch_size'],\n",
    "                                  window_size=window_size,\n",
    "                                  matrix=train_sequence,\n",
    "                                  target=train_target,\n",
    "                                  window_index_list=train_indices_downsampled)\n",
    "\n",
    "val_gen = MovingWindowGenerator(batch_size=hyperparameter_dict['batch_size'],\n",
    "                                window_size=window_size,\n",
    "                                matrix=validation_sequence,\n",
    "                                target=validation_target,\n",
    "                                window_index_list=validation_indices_downsampled)\n",
    "\n",
    "# The input shape of the network, not including the batch dimension\n",
    "hyperparameter_dict['input_dim'] = (window_size, n_features)\n",
    "\n",
    "# Construct a model using the hyper parameter dictionary\n",
    "model = build_simple_CNN(hyperparameter_dict)\n",
    "\n",
    "optimizer = Adam(lr=hyperparameter_dict['learning_rate'])\n",
    "\n",
    "assert(n_classes > 1)\n",
    "\n",
    "loss = 'sparse_categorical_crossentropy' if n_classes > 2 else 'binary_crossentropy'\n",
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "timestamp = time.time()\n",
    "model_file_name = f'model-{timestamp}'\n",
    "save_dir = f'model-{timestamp}'\n",
    "file_name = 'best_model-{epoch:02d}-{val_acc:.3f}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=f'{save_dir}/{file_name}.model',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=hyperparameter_dict['monitor'],\n",
    "                               min_delta=0,\n",
    "                               patience=hyperparameter_dict['patience'],\n",
    "                               verbose=0)\n",
    "\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "history = model.fit_generator(generator=train_gen,\n",
    "                              validation_data=val_gen,\n",
    "                              epochs=hyperparameter_dict['n_epochs'],\n",
    "                              use_multiprocessing=False)\n",
    "\n",
    "val_gen_unbalanced = MovingWindowGenerator(batch_size=hyperparameter_dict['batch_size'],\n",
    "                                           window_size=window_size,\n",
    "                                           matrix=validation_sequence,\n",
    "                                           target=validation_target,\n",
    "                                           window_index_list=validation_indices_unbalanced)\n",
    "\n",
    "probabilities = model.predict_generator(generator=val_gen_unbalanced).squeeze(-1)\n",
    "predictions = [1 if p > 0.5 else 0 for p in probabilities]  # Convert probabilities into decisions\n",
    "\n",
    "l=len(predictions)\n",
    "y = val_gen_unbalanced.classes  # Extract the correct answers from the generator\n",
    "y_=y[0:l]\n",
    "\n",
    "\n",
    "y = val_gen_unbalanced.classes  # Extract the correct answers from the generator\n",
    "acc = accuracy_score(y_, predictions)\n",
    "cm = confusion_matrix(y_, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (fp + tp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "json.dump(hyperparameter_dict, open(f'{save_dir}/hyperparameters_dictionary.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df=pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.697954</td>\n",
       "      <td>0.541977</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0.521789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.705181</td>\n",
       "      <td>0.527015</td>\n",
       "      <td>0.689952</td>\n",
       "      <td>0.544782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.710396</td>\n",
       "      <td>0.533131</td>\n",
       "      <td>0.684429</td>\n",
       "      <td>0.558787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.713070</td>\n",
       "      <td>0.561162</td>\n",
       "      <td>0.674347</td>\n",
       "      <td>0.582076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.690925</td>\n",
       "      <td>0.530856</td>\n",
       "      <td>0.646837</td>\n",
       "      <td>0.622397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.777756</td>\n",
       "      <td>0.534732</td>\n",
       "      <td>0.590968</td>\n",
       "      <td>0.678586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.765998</td>\n",
       "      <td>0.541738</td>\n",
       "      <td>0.514881</td>\n",
       "      <td>0.737375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.525898</td>\n",
       "      <td>0.435317</td>\n",
       "      <td>0.789308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.951757</td>\n",
       "      <td>0.529822</td>\n",
       "      <td>0.361648</td>\n",
       "      <td>0.832452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.868003</td>\n",
       "      <td>0.524859</td>\n",
       "      <td>0.301108</td>\n",
       "      <td>0.865905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.152638</td>\n",
       "      <td>0.521383</td>\n",
       "      <td>0.252086</td>\n",
       "      <td>0.890696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.267503</td>\n",
       "      <td>0.532731</td>\n",
       "      <td>0.213665</td>\n",
       "      <td>0.909630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.159563</td>\n",
       "      <td>0.519012</td>\n",
       "      <td>0.183547</td>\n",
       "      <td>0.924326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.145244</td>\n",
       "      <td>0.524913</td>\n",
       "      <td>0.159940</td>\n",
       "      <td>0.936081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.552147</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>0.142494</td>\n",
       "      <td>0.943359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  val_accuracy      loss  accuracy\n",
       "0   0.697954      0.541977  0.715415  0.521789\n",
       "1   0.705181      0.527015  0.689952  0.544782\n",
       "2   0.710396      0.533131  0.684429  0.558787\n",
       "3   0.713070      0.561162  0.674347  0.582076\n",
       "4   0.690925      0.530856  0.646837  0.622397\n",
       "5   0.777756      0.534732  0.590968  0.678586\n",
       "6   0.765998      0.541738  0.514881  0.737375\n",
       "7   0.884817      0.525898  0.435317  0.789308\n",
       "8   0.951757      0.529822  0.361648  0.832452\n",
       "9   0.868003      0.524859  0.301108  0.865905\n",
       "10  1.152638      0.521383  0.252086  0.890696\n",
       "11  1.267503      0.532731  0.213665  0.909630\n",
       "12  1.159563      0.519012  0.183547  0.924326\n",
       "13  1.145244      0.524913  0.159940  0.936081\n",
       "14  1.552147      0.517232  0.142494  0.943359"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_json_file='history.json'\n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.to_pickle(\"resultados.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('y_.pickle', 'wb') as f:\n",
    "    pickle.dump(y_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=pd.DataFrame(probabilities)\n",
    "prob.to_pickle(\"probabilities.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=pd.DataFrame(cm)\n",
    "conf.to_pickle(\"confusion.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(predictions)\n",
    "pred.to_pickle(\"predictions.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4625859542328937"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5301123885802868"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5171875"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ce36239fc04e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(model.history.history['val_accuracy'])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
