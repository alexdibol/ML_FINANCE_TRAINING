{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  BITCOIN PRICE FORECASTING </center></h1>\n",
    "\n",
    "<h2><center>LONG VERSION</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CODE FOR A 15 EPOCH CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "961/961 [==============================] - 498s 518ms/step - loss: 0.7156 - accuracy: 0.5228 - val_loss: 0.6844 - val_accuracy: 0.5551\n",
      "Epoch 2/15\n",
      "961/961 [==============================] - 503s 523ms/step - loss: 0.6904 - accuracy: 0.5445 - val_loss: 0.6823 - val_accuracy: 0.5618\n",
      "Epoch 3/15\n",
      "961/961 [==============================] - 514s 535ms/step - loss: 0.6849 - accuracy: 0.5556 - val_loss: 0.7036 - val_accuracy: 0.5462\n",
      "Epoch 4/15\n",
      "961/961 [==============================] - 522s 544ms/step - loss: 0.6779 - accuracy: 0.5743 - val_loss: 0.7031 - val_accuracy: 0.5532\n",
      "Epoch 5/15\n",
      "961/961 [==============================] - 522s 543ms/step - loss: 0.6572 - accuracy: 0.6090 - val_loss: 0.7151 - val_accuracy: 0.5293\n",
      "Epoch 6/15\n",
      "961/961 [==============================] - 522s 543ms/step - loss: 0.6089 - accuracy: 0.6618 - val_loss: 0.7354 - val_accuracy: 0.5482\n",
      "Epoch 7/15\n",
      "961/961 [==============================] - 523s 545ms/step - loss: 0.5327 - accuracy: 0.7234 - val_loss: 0.7647 - val_accuracy: 0.5374\n",
      "Epoch 8/15\n",
      "961/961 [==============================] - 522s 543ms/step - loss: 0.4503 - accuracy: 0.7783 - val_loss: 0.8642 - val_accuracy: 0.5445\n",
      "Epoch 9/15\n",
      "961/961 [==============================] - 522s 543ms/step - loss: 0.3756 - accuracy: 0.8221 - val_loss: 0.9605 - val_accuracy: 0.5335\n",
      "Epoch 10/15\n",
      "961/961 [==============================] - 523s 544ms/step - loss: 0.3141 - accuracy: 0.8567 - val_loss: 1.0198 - val_accuracy: 0.5224\n",
      "Epoch 11/15\n",
      "961/961 [==============================] - 524s 545ms/step - loss: 0.2671 - accuracy: 0.8815 - val_loss: 1.0625 - val_accuracy: 0.5306\n",
      "Epoch 12/15\n",
      "961/961 [==============================] - 522s 543ms/step - loss: 0.2288 - accuracy: 0.9008 - val_loss: 1.1800 - val_accuracy: 0.5263\n",
      "Epoch 13/15\n",
      "961/961 [==============================] - 530s 552ms/step - loss: 0.1994 - accuracy: 0.9154 - val_loss: 1.3551 - val_accuracy: 0.5273\n",
      "Epoch 14/15\n",
      "961/961 [==============================] - 526s 548ms/step - loss: 0.1757 - accuracy: 0.9269 - val_loss: 1.3170 - val_accuracy: 0.5291\n",
      "Epoch 15/15\n",
      "961/961 [==============================] - 524s 545ms/step - loss: 0.1553 - accuracy: 0.9365 - val_loss: 1.5328 - val_accuracy: 0.5323\n",
      "accuracy 0.53251953125\n",
      "precision 0.47179058600223805\n",
      "recall 0.4303452374799731\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def exponential_moving_average(ps, look_back):\n",
    "    return ps.ewm(span=look_back, min_periods=0, adjust=False, ignore_na=True).mean()\n",
    "\n",
    "\n",
    "def macd(ps, short, long):\n",
    "    short_ema = exponential_moving_average(ps, short)\n",
    "    long_ema = exponential_moving_average(ps, long)\n",
    "    return short_ema - long_ema\n",
    "\n",
    "\n",
    "def log_difference(ps):\n",
    "    return log_momentum(ps, 1)\n",
    "\n",
    "\n",
    "def log_momentum(ps, m):\n",
    "    log_price = np.log(ps)\n",
    "    shifted_log_price = log_price.shift(m)\n",
    "    return log_price - shifted_log_price\n",
    "\n",
    "\n",
    "class MovingWindowGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, window_size, matrix, target, window_index_list, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.window_index_list = window_index_list\n",
    "        self.example_index_list = np.arange(len(self.window_index_list))\n",
    "        self.matrix = matrix\n",
    "        self.target = target\n",
    "        self.window_size = window_size  # The size of the moving window\n",
    "        self.on_epoch_end()  # Randomize the sample order\n",
    "        self.classes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.window_index_list) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example_indices_for_current_batch = self.example_index_list[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self.__data_generation(example_indices_for_current_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.example_index_list)\n",
    "\n",
    "    def __data_generation(self, example_indexs_for_current_batch):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        n = self.window_size\n",
    "\n",
    "        for sample_index in example_indexs_for_current_batch:\n",
    "            window_index = self.window_index_list[sample_index]\n",
    "            window = self.matrix[window_index-n+1:window_index+1]  # Extract the window\n",
    "            target = self.target[window_index]  # Extract the target(y)\n",
    "            X.append(window)\n",
    "            y.append(target)\n",
    "\n",
    "        self.classes += y\n",
    "\n",
    "        # Convert python lists to numpy arrays\n",
    "        X, y = np.stack(X), np.stack(y)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def add_fully_connected_layers(model, hyperparameter_dict):\n",
    "    fc_layers = hyperparameter_dict['fc_layers']\n",
    "    activation = hyperparameter_dict['activation']\n",
    "    n_classes = hyperparameter_dict['n_classes']\n",
    "\n",
    "    if fc_layers is not None:\n",
    "        for layer in fc_layers:\n",
    "            model.add(Dense(layer, activation=activation))\n",
    "            if 'dropout' in hyperparameter_dict:\n",
    "                model.add(Dropout(hyperparameter_dict['dropout']))\n",
    "\n",
    "            if 'batch_normalization' in hyperparameter_dict:\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "    if n_classes > 2:\n",
    "        model.add(Dense(n_classes, activation='softmax'))\n",
    "    elif n_classes == 2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    else:  # Regression\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_simple_CNN(hyperparameter_dict):\n",
    "    model = Sequential()\n",
    "    kernel_size = hyperparameter_dict['kernel_size']\n",
    "    n_filters = hyperparameter_dict['n_filters']\n",
    "    n_layers = hyperparameter_dict['n_cnn_layers']\n",
    "    n_blocks = hyperparameter_dict['n_cnn_blocks']\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "        for j in range(n_layers):\n",
    "            params = {\n",
    "                \"filters\": n_filters,\n",
    "                \"kernel_size\": kernel_size,\n",
    "                \"activation\": hyperparameter_dict['activation'],\n",
    "                \"padding\": 'same'\n",
    "            }\n",
    "\n",
    "            if i == 0 and j == 0:  # Handle the first layer specially\n",
    "                params['input_shape'] = hyperparameter_dict['input_dim']\n",
    "\n",
    "            model.add(Conv1D(**params))\n",
    "\n",
    "            if 'cnn_dropout' in hyperparameter_dict:\n",
    "                model.add(Dropout(hyperparameter_dict['cnn_dropout']))\n",
    "\n",
    "            if 'cnn_bn' in hyperparameter_dict:\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "        if 'pool_size' in hyperparameter_dict:\n",
    "            model.add(MaxPooling1D(pool_size=hyperparameter_dict['pool_size']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    add_fully_connected_layers(model, hyperparameter_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def balance_and_generate_index_lists(sequence, target_series, window_size):\n",
    "    window_index_dictionary = defaultdict(list)\n",
    "    unbalanced_window_index_list = []\n",
    "    downsampled_window_index_list = []\n",
    "\n",
    "    # Populate the dictionary with window indices\n",
    "    for i in range(window_size - 1, len(sequence) - window_size + 1):\n",
    "        window_index_dictionary[target_series[i]].append(i)\n",
    "\n",
    "    # Create an unbalanced version of the index list\n",
    "    for cls in window_index_dictionary:\n",
    "        unbalanced_window_index_list += window_index_dictionary[cls]\n",
    "\n",
    "    # Figure out the least frequent class\n",
    "    lengths = [len(index_list_for_class) for index_list_for_class in window_index_dictionary.values()]\n",
    "    min_length = int(np.min(lengths))\n",
    "\n",
    "    # Undersampling happens here\n",
    "    for cls in window_index_dictionary:\n",
    "        random.shuffle(window_index_dictionary[cls])\n",
    "        window_index_dictionary[cls] = window_index_dictionary[cls][:min_length]\n",
    "\n",
    "    # Concatenate the list\n",
    "    for cls in window_index_dictionary:\n",
    "        downsampled_window_index_list += window_index_dictionary[cls]\n",
    "\n",
    "    return downsampled_window_index_list, unbalanced_window_index_list\n",
    "\n",
    "\n",
    "hyperparameter_dict = {\n",
    "    'kernel_size': 5,\n",
    "    'dropout': 0.2,\n",
    "    'n_classes': 2,\n",
    "    'window_size': 100,\n",
    "    'validation_ratio': 0.25,\n",
    "    'batch_size': 512,\n",
    "    'n_epochs': 15,\n",
    "    'monitor': 'val_loss',\n",
    "    'patience': 5,\n",
    "\n",
    "    'forecast_horizon': 10,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    # model-specific hyper parameters\n",
    "    'model': 'stacked_CNN',\n",
    "    'n_cnn_layers': 2,\n",
    "    'n_cnn_blocks': 1,\n",
    "    'cnn_bn': True,\n",
    "    'batch_normalization': True,\n",
    "    'n_filters': 64,\n",
    "    'fc_layers': [1024, 1024],\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "# Extracting values from the hyperparameter dictionary\n",
    "window_size = hyperparameter_dict['window_size']\n",
    "forecast_horizon = hyperparameter_dict['forecast_horizon']\n",
    "n_epochs = hyperparameter_dict['n_epochs']\n",
    "n_classes = hyperparameter_dict['n_classes']\n",
    "validation_ratio = hyperparameter_dict['validation_ratio']\n",
    "\n",
    "df = pd.read_csv('NEOBTC.csv', index_col='timestamp', parse_dates=True)\n",
    "\n",
    "# Optional Indicator Calculation\n",
    "price = df['close']\n",
    "df['MACD'] = macd(price, 10, 20)\n",
    "df['log_difference'] = log_difference(price)\n",
    "df['log_momentum'] = log_momentum(price, 20)\n",
    "\n",
    "# Target Generation\n",
    "df['shifted_close'] = df['close'].shift(-forecast_horizon)  # The shifted version will have a couple of NaNs\n",
    "df.dropna(inplace=True)  # Drop the NaNs\n",
    "df['target'] = (df['shifted_close'] - df['close'] > 0).astype(int)\n",
    "df.drop(['shifted_close'], axis=1, inplace=True)  # We don't need this column any more\n",
    "\n",
    "# Isolate the target series\n",
    "target_series = df['target'].values  # Extract the numpy array inside\n",
    "df.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "# Split\n",
    "n_train = len(df) - int(validation_ratio * len(df))\n",
    "train_df, validation_df = df[:n_train], df[n_train:]\n",
    "train_target, validation_target = target_series[:n_train], target_series[n_train:]\n",
    "\n",
    "n_features = train_df.shape[1]\n",
    "\n",
    "# Standardize the matrices\n",
    "scaler = StandardScaler()\n",
    "train_sequence = scaler.fit_transform(train_df)\n",
    "validation_sequence = scaler.transform(validation_df)\n",
    "\n",
    "# Downsample and generate moving window indices\n",
    "train_indices_downsampled, train_indices_unbalanced = balance_and_generate_index_lists(train_sequence,\n",
    "                                                                                       train_target,\n",
    "                                                                                       window_size)\n",
    "validation_indices_downsampled, validation_indices_unbalanced = balance_and_generate_index_lists(validation_sequence,\n",
    "                                                                                                 train_target,\n",
    "                                                                                                 window_size)\n",
    "\n",
    "# Instantiate the moving window generators for each partition\n",
    "train_gen = MovingWindowGenerator(batch_size=hyperparameter_dict['batch_size'],\n",
    "                                  window_size=window_size,\n",
    "                                  matrix=train_sequence,\n",
    "                                  target=train_target,\n",
    "                                  window_index_list=train_indices_downsampled)\n",
    "\n",
    "val_gen = MovingWindowGenerator(batch_size=hyperparameter_dict['batch_size'],\n",
    "                                window_size=window_size,\n",
    "                                matrix=validation_sequence,\n",
    "                                target=validation_target,\n",
    "                                window_index_list=validation_indices_downsampled)\n",
    "\n",
    "# The input shape of the network, not including the batch dimension\n",
    "hyperparameter_dict['input_dim'] = (window_size, n_features)\n",
    "\n",
    "# Construct a model using the hyper parameter dictionary\n",
    "model = build_simple_CNN(hyperparameter_dict)\n",
    "\n",
    "optimizer = Adam(lr=hyperparameter_dict['learning_rate'])\n",
    "\n",
    "assert(n_classes > 1)\n",
    "\n",
    "loss = 'sparse_categorical_crossentropy' if n_classes > 2 else 'binary_crossentropy'\n",
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "timestamp = time.time()\n",
    "model_file_name = f'model-{timestamp}'\n",
    "save_dir = f'model-{timestamp}'\n",
    "file_name = 'best_model-{epoch:02d}-{val_acc:.3f}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=f'{save_dir}/{file_name}.model',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=hyperparameter_dict['monitor'],\n",
    "                               min_delta=0,\n",
    "                               patience=hyperparameter_dict['patience'],\n",
    "                               verbose=0)\n",
    "\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "history = model.fit_generator(generator=train_gen,\n",
    "                              validation_data=val_gen,\n",
    "                              epochs=hyperparameter_dict['n_epochs'],\n",
    "                              use_multiprocessing=False)\n",
    "\n",
    "val_gen_unbalanced = MovingWindowGenerator(batch_size=hyperparameter_dict['batch_size'],\n",
    "                                           window_size=window_size,\n",
    "                                           matrix=validation_sequence,\n",
    "                                           target=validation_target,\n",
    "                                           window_index_list=validation_indices_unbalanced)\n",
    "\n",
    "probabilities = model.predict_generator(generator=val_gen_unbalanced).squeeze(-1)\n",
    "predictions = [1 if p > 0.5 else 0 for p in probabilities]  # Convert probabilities into decisions\n",
    "\n",
    "l=len(predictions)\n",
    "y = val_gen_unbalanced.classes  # Extract the correct answers from the generator\n",
    "y_=y[0:l]\n",
    "\n",
    "\n",
    "y = val_gen_unbalanced.classes  # Extract the correct answers from the generator\n",
    "acc = accuracy_score(y_, predictions)\n",
    "cm = confusion_matrix(y_, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (fp + tp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print('accuracy', acc)\n",
    "print('precision', precision)\n",
    "print('recall', recall)\n",
    "\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "json.dump(hyperparameter_dict, open(f'{save_dir}/hyperparameters_dictionary.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df=pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.684372</td>\n",
       "      <td>0.555094</td>\n",
       "      <td>0.715610</td>\n",
       "      <td>0.522842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.682296</td>\n",
       "      <td>0.561783</td>\n",
       "      <td>0.690360</td>\n",
       "      <td>0.544511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703615</td>\n",
       "      <td>0.546152</td>\n",
       "      <td>0.684937</td>\n",
       "      <td>0.555610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.703085</td>\n",
       "      <td>0.553176</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.574255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.715100</td>\n",
       "      <td>0.529321</td>\n",
       "      <td>0.657245</td>\n",
       "      <td>0.608954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.735428</td>\n",
       "      <td>0.548165</td>\n",
       "      <td>0.608937</td>\n",
       "      <td>0.661833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.764652</td>\n",
       "      <td>0.537438</td>\n",
       "      <td>0.532742</td>\n",
       "      <td>0.723388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.864169</td>\n",
       "      <td>0.544450</td>\n",
       "      <td>0.450316</td>\n",
       "      <td>0.778338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.960483</td>\n",
       "      <td>0.533478</td>\n",
       "      <td>0.375552</td>\n",
       "      <td>0.822132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.019834</td>\n",
       "      <td>0.522386</td>\n",
       "      <td>0.314078</td>\n",
       "      <td>0.856735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.062518</td>\n",
       "      <td>0.530575</td>\n",
       "      <td>0.267119</td>\n",
       "      <td>0.881451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.180001</td>\n",
       "      <td>0.526287</td>\n",
       "      <td>0.228785</td>\n",
       "      <td>0.900813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.355056</td>\n",
       "      <td>0.527302</td>\n",
       "      <td>0.199442</td>\n",
       "      <td>0.915369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.316964</td>\n",
       "      <td>0.529130</td>\n",
       "      <td>0.175730</td>\n",
       "      <td>0.926936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.532790</td>\n",
       "      <td>0.532259</td>\n",
       "      <td>0.155273</td>\n",
       "      <td>0.936492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  val_accuracy      loss  accuracy\n",
       "0   0.684372      0.555094  0.715610  0.522842\n",
       "1   0.682296      0.561783  0.690360  0.544511\n",
       "2   0.703615      0.546152  0.684937  0.555610\n",
       "3   0.703085      0.553176  0.677900  0.574255\n",
       "4   0.715100      0.529321  0.657245  0.608954\n",
       "5   0.735428      0.548165  0.608937  0.661833\n",
       "6   0.764652      0.537438  0.532742  0.723388\n",
       "7   0.864169      0.544450  0.450316  0.778338\n",
       "8   0.960483      0.533478  0.375552  0.822132\n",
       "9   1.019834      0.522386  0.314078  0.856735\n",
       "10  1.062518      0.530575  0.267119  0.881451\n",
       "11  1.180001      0.526287  0.228785  0.900813\n",
       "12  1.355056      0.527302  0.199442  0.915369\n",
       "13  1.316964      0.529130  0.175730  0.926936\n",
       "14  1.532790      0.532259  0.155273  0.936492"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_json_file='history.json'\n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.to_pickle(\"resultados.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
