{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> PROPENSITY TO INVEST </center></h1>\n",
    "<h2><center> ENSEMBLE CLASSIFICATION METHODS </center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VOTING CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner. The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.\n",
    "\n",
    "Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model, so ensembles may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. Fast algorithms such as decision trees are commonly used in ensemble methods (for example, random forests), although slower algorithms can benefit from ensemble techniques as well.\n",
    "\n",
    "By analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection.\n",
    "\n",
    "An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data.\n",
    "\n",
    "Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine.Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).[9] Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PREDICTING REAL ESTATE BUYER'S LEVEL OF INTEREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may well know in most Kaggle competitions the winners usually resort to stacking or meta-ensembling, which is a technique involving the combination of several 1st level predictive models to generate a 2nd level model which tends to outperform all of them.\n",
    "\n",
    "This usually happens because the 2nd level model is somewhat able to exploit the strengths of each 1st level model where they perform best, while smoothing the impact of their weaknesses in other parts of the dataset.\n",
    "\n",
    "There are different methods and \"schools of thought\" on how stacking can be performed. If you are interested in this topic, then I suggest you to have a look at this and this to start.\n",
    "\n",
    "Here I will show a simple technique that is known as \"Soft Voting\" and can be implemented with Sklearn VotingClassifier.\n",
    "\n",
    "More can be found at https://www.kaggle.com/den3b81/better-predictions-stacking-with-votingclassifier/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier \n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(open(\"train.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"num_photos\"] = df[\"photos\"].apply(len)\n",
    "df[\"num_features\"] = df[\"features\"].apply(len)\n",
    "df[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"created\"] = pd.to_datetime(df[\"created\"])\n",
    "df[\"created_year\"] = df[\"created\"].dt.year\n",
    "df[\"created_month\"] = df[\"created\"].dt.month\n",
    "df[\"created_day\"] = df[\"created\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>num_photos</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_description_words</th>\n",
       "      <th>created_year</th>\n",
       "      <th>created_month</th>\n",
       "      <th>created_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.7108</td>\n",
       "      <td>-73.9539</td>\n",
       "      <td>2400</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>77</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7513</td>\n",
       "      <td>-73.9722</td>\n",
       "      <td>3800</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>131</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.7575</td>\n",
       "      <td>-73.9625</td>\n",
       "      <td>3495</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>119</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7145</td>\n",
       "      <td>-73.9425</td>\n",
       "      <td>3000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7439</td>\n",
       "      <td>-73.9743</td>\n",
       "      <td>2795</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bathrooms  bedrooms  latitude  longitude  price  num_photos  num_features  \\\n",
       "4         1.0         1   40.7108   -73.9539   2400          12             7   \n",
       "6         1.0         2   40.7513   -73.9722   3800           6             6   \n",
       "9         1.0         2   40.7575   -73.9625   3495           6             6   \n",
       "10        1.5         3   40.7145   -73.9425   3000           5             0   \n",
       "15        1.0         0   40.7439   -73.9743   2795           4             4   \n",
       "\n",
       "    num_description_words  created_year  created_month  created_day  \n",
       "4                      77          2016              6           16  \n",
       "6                     131          2016              6            1  \n",
       "9                     119          2016              6           14  \n",
       "10                     95          2016              6           24  \n",
       "15                     41          2016              6           28  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feats = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n",
    "             \"num_photos\", \"num_features\", \"num_description_words\",\n",
    "             \"created_year\", \"created_month\", \"created_day\"]\n",
    "X = df[num_feats]\n",
    "y = df[\"interest_level\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state for reproducing same results\n",
    "random_state = 54321\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state = 54321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this little experiment we will combine the predictions of 4 different classifiers, trained with basic parametrization. Tuning the parameters is beyond the scope of this notebook.\n",
    "\n",
    "The classifiers are:\n",
    "\n",
    ">-  RandomForestClassifier with \"entropy\" criterion \n",
    ">-  RandomForestClassifier with \"gini\" criterion \n",
    ">-  Sklearn GradientBoostingClassifier \n",
    ">-  XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6307891949598801"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=250, criterion='entropy',  n_jobs = -1,  random_state=random_state)\n",
    "rf1.fit(X_train, y_train)\n",
    "y_val_pred = rf1.predict_proba(X_val)\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.632009390517281"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2= RandomForestClassifier(n_estimators=250, criterion='gini',  n_jobs = -1, random_state=random_state)\n",
    "rf2.fit(X_train, y_train)\n",
    "y_val_pred = rf2.predict_proba(X_val)\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6392365307706523"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "gbc.fit(X_train, y_train)\n",
    "y_val_pred = gbc.predict_proba(X_val)\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6479794418970433"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(seed=random_state)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_val_pred = xgb.predict_proba(X_val)\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOFT VOTING\n",
    "\n",
    "It looks like rf1 tops all the other classifiers, with XGB coming last (as I said before, we are not concerned with tuning the parameters).\n",
    "\n",
    "Let's now see whether we can improve performances by combining model predictions through \"Soft Voting\". Soft voting entails computing a weighed sum of the predicted probabilities of all models for each class. If the weights are equal for all classifiers, then the probabilities are simply the averages for each class.\n",
    "\n",
    "The predicted label is the argmax of the summed prediction probabilities.\n",
    "\n",
    "Soft voting can be easily performed using a VotingClassifier, which will \"retrain\" the model prototypes we specified before. Please note that one could avoid retraining the models (in this case) and manually perform the soft voting using the output of each classifier's \"predic_proba\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6228273354540116"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf = VotingClassifier(estimators=[\n",
    "    ('rf1', rf1), ('rf2', rf2), ('gbc', gbc), ('xgb',xgb)], voting='soft')\n",
    "eclf.fit(X_train, y_train)\n",
    "y_val_pred = eclf.predict_proba(X_val)\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOFT VOTING WITH WEIGTHED COEFFICIENTS\n",
    "\n",
    "We can already see a decent improvement with respect to the best performing model. We could improve further by using a different coefficients for each classifier, based on their performances.\n",
    "\n",
    "For instance, it is common to assign greater weight to the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6205717377933158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf = VotingClassifier(estimators=[\n",
    "    ('rf1', rf1), ('rf2', rf2), ('gbc', gbc), ('xgb',xgb)], voting='soft', weights = [3,1,1,1])\n",
    "eclf.fit(X_train, y_train)\n",
    "y_val_pred = eclf.predict_proba(X_val)\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
